---
title: "Lab 4: Advanced Data Wrangling"
output: html_document
---

## Tidy Data

#### 1. Examine the built-in dataset `ChickWeight`. Which of the following is true:

* ChickWeight is not tidy: each chick has more than one row.
**ChickWeight is tidy: each observation (a weight) is represented by one row. The chick from which this measurement came is one of the variables.**
* ChickWeight is not tidy: we are missing the year column.
* ChickWeight is tidy: it is stored in a data frame.

```{r}
library(tidyverse)
View(ChickWeight)

ChickWeight %>% 
  pivot_wider(names_from = Time,
              values_from = weight) %>% 
  View()

ggplot(ChickWeight, aes(x = Time, y = weight)) +
  geom_point(aes(color = as.factor(Chick)))

```

#### 2. Examine the built-in dataset BOD. Which of the following is true:

* BOD is not tidy: it only has six rows.
* BOD is not tidy: the first column is just an index.
** BOD is tidy: each row is an observation with two values (time and demand) **
* BOD is tidy: all small datasets are tidy by definition.

```{r}
View(BOD)
```

## Reshaping Data and Joining Tables

#### 3. Run the following command to define the co2_wide object:

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
data(co2)
co2_wide <- data.frame(matrix(co2, ncol = 12, byrow = TRUE)) %>% 
  setNames(1:12) %>%
  mutate(year = as.character(1959:1997))
```

Use the `pivot_longer` function to wrangle this into a tidy dataset. Call the column with the CO2 measurements `co2` and call the month column `month`. Call the resulting object `co2_tidy`.

```{r}
### 2 equivalent ways to do the same thing
co2_tidy <- 
  co2_wide %>% 
  pivot_longer(cols = `1`:`12`,
               names_to = 'month',
               values_to = 'co2')

co2_tidy <- 
  co2_wide %>% 
  pivot_longer(cols = -year,
               names_to = 'month',
               values_to = 'co2')

```


#### 4. Plot CO2 versus month with a different curve for each year using this code:

```{r}
### w/ Group
ggplot(co2_tidy, aes(x = as.numeric(month), y = co2)) + 
  geom_line(aes(group = year))

### w/ color
ggplot(co2_tidy, aes(x = as.numeric(month), y = co2)) + 
  geom_line(aes(color = year))

### w/ color as continuous
ggplot(co2_tidy, aes(x = as.numeric(month), y = co2)) + 
  geom_line(aes(color = as.numeric(year), group = year))

```

If the expected plot is not made, it is probably because `co2_tidy$month` is not numeric:

```{r}
class(co2_tidy$month)
```

Rewrite your code to make sure the `month` column is numeric. Then remake the plot.

```{r}
co2_tidy <- 
  co2_tidy %>% 
  mutate('month'= as.numeric(month),
         'year' = as.numeric(year))

```


#### 5. What do we learn from this plot?

* CO2 measures increase monotonically from 1959 to 1997.
* CO2 measures are higher in the summer and the yearly average increased from 1959 to 1997.
* CO2 measures appear constant and random variability explains the differences.
* CO2 measures do not have a seasonal trend.

#### 6. We want to see if the monthly trend is changing so we are going to remove the year effects and then plot the results. We will first compute the year averages. Use `group_by` and `summarize` to compute the average co2 for each year. Save in an object called `yearly_avg`.

```{r}
yearly_avg <- 
  co2_tidy %>% 
  group_by(year) %>% 
  summarize('avg_co2' = mean(co2))


```

#### 7. Now use the `left_join` function to add the yearly average to the `co2_tidy` dataset. Then compute the residuals: observed co2 measure - yearly average.

```{r}
### Doing this w/out any joins
co2_tidy %>% 
  group_by(year) %>% 
  mutate('avg_co2' = mean(co2)) 

### The way the problem asked
co2_tidy_2 <- 
  co2_tidy %>% 
  left_join(yearly_avg, by = 'year')

co2_tidy_2 <- 
  co2_tidy_2 %>% 
  mutate('residual' = co2 - avg_co2)


```


#### 8. Make a plot of the seasonal trends by year but only after removing the year effect.

```{r}

ggplot(co2_tidy_2, aes(x = month, y = residual)) + 
  geom_line(aes(color = year, group = year))

```
---

#### 9. Install and load the `Lahman` library. This database includes data related to baseball teams. It includes summary statistics about how the players performed on offense and defense for several years. It also includes personal information about the players.

The `Batting` data frame contains the offensive statistics for all players for many years. You can see, for example, the top 10 hitters in 2016 by running this code:

```{r}
#install.packages('Lahman')
library(Lahman)

top <- Batting %>% 
  filter(yearID == 2016) %>%
  arrange(desc(HR)) %>%
  slice(1:10)

head(top)
```


But who are these players? We see an ID, but not the names. The player names are in this table

```{r}
People %>% head()
```

We can see column names `nameFirst` and `nameLast`. Use the `left_join` function to create a table of the top home run hitters. The table should have playerID, first name, last name, and number of home runs (HR). Rewrite the object `top` with this new table.

```{r}
### Select name related columns
people_small <- 
  People %>% 
  select(playerID, nameFirst, nameLast)


### Join
top <- 
  top %>% 
  left_join(people_small, by = 'playerID') 

```

#### 10. Now use the `Salaries` data frame to add each playerâ€™s salary to the table you created in exercise 6. Note that salaries are different every year so make sure to filter for the year 2016, then use `right_join`. This time show first name, last name, team, HR, and salary.

```{r}
Salaries %>% 
  select(playerID, yearID, salary) %>% 
  right_join(top, by = c('playerID', 'yearID')) %>% 
  select(nameFirst, nameLast, teamID, HR, salary)

```


## Parsing Dates and Times

#### 11. First run the code below to create a data frame called `cases_df`. This code involved web scraping - which we haven't covered in class yet - so don't worry about what the code is doing for now and just trust us. This data frame includes the date of the first swine flu case in each country.

```{r, message=FALSE, echo=FALSE}
library(rvest)
# Wikipedia article to scrape
url <- "https://en.wikipedia.org/w/index.php?title=2009_swine_flu_pandemic_tables&oldid=950511922"

# If you are unable to access Wikipedia, uncomment and use the following 
# line to read in the saved HTML file of the webpage
# url = "h1n1_wiki_tables.html"

# Extract all tables in the page
tab <- read_html(url) %>% html_nodes("table")

# In the latest version of rvest, you can use html_elements instead of html_nodes to read in a node set
tab

# Access the first node in the node set, convert table to list of 1 dataframe, then extract data frame
cases_df <- tab %>% .[1] %>% html_table %>% .[[1]]

# Variable names to use for the table of case counts
case_names <- c("by_date", "by_continent", "country", "first_case",
                "April", "May", "June", "July", "August", "latest")

cases_df <- tab %>% .[1] %>% html_table %>% .[[1]] %>% setNames(case_names)
```


Calling the `str()` function, we can see that the column for the date of the first case is a string, which means that R understands the contents as a set of characters and not as a date. So if we try to order our data frame by the date of the first case, R won't do a good job.

```{r, echo=FALSE}
str(cases_df)
```

But we can use the function `ymd()` to convert it into a date. Then we can use the converted date column to arrange our data frame according to the date of the first case.

##### a) Convert the `first_case` column to a date and arrange the data frame by the `first_case` date.

```{r, message=FALSE}
library(lubridate)

cases_df <- 
  cases_df %>% 
  mutate('first_case' = ymd(first_case))



```

##### b) Calculate the time difference between each country's first case and the previous country's first case using the `lag()` function, and save it into a column called `first_case_diff`.

```{r, message=FALSE}

cases_df %>% 
  mutate('first_case_diff' = first_case - lag(first_case)) %>% 
  View()



x <- c(3, 5, 10, 20)
x - lag(x)


```

