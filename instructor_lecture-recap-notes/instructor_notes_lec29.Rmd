---
title: "instructor recap notes - lec 29"
date: "2023-12-04"
output: html_document
---

```{r setup, message=FALSE}
rm(list=ls())
# load the required libraries
library(tidyverse)
library(ggplot2)
library(dslabs)
library(broom)
library(caret)
library(tree)
library(rpart)
library(MASS)
library(randomForest)
library(datasets)
library(doParallel)

```
## Random quiz: 
For KNN, will the model be more generalization or less generalizable with increased $k$?

**Tip of the day**
`caret` package stands for "Classification And REgression Training" and it supports parallel computing,  meaning you can train models faster by utilizing multiple cores of your processor. 

```{r, warning=FALSE, message=FALSE}
# Install (if you have not) and load required packages
# install.packages("doParallel")
# library(caret)
# library(doParallel)


detectCores()
# Register parallel backend to use multiple cores
registerDoParallel(cores = 4)  # Adjust the number of cores according to your machine

# Check if a parallel backend is registered
if (!is.null(getDoParRegistered()) && getDoParRegistered()) {
    current_cores <- getDoParWorkers()
} else {
    current_cores <- 0  # No parallel backend registered
}
current_cores

# Train a model with parallel processing
ts = Sys.time()
modelFit <- train(Species ~ ., data = iris, method = "rf", trControl = trainControl(method = "cv", number = 100))
te = Sys.time()
te - ts

# Stop parallel processing
stopImplicitCluster()
registerDoSEQ()  # This unregisters the parallel backend and reverts to sequential execution

# Check if a parallel backend is unregistered
if (!is.null(getDoParRegistered()) && getDoParRegistered()) {
    current_cores <- getDoParWorkers()
} else {
    current_cores <- 0  # No parallel backend registered
}
current_cores

# Train a model without parallel processing
ts = Sys.time()
modelFit <- train(Species ~ ., data = iris, method = "rf", trControl = trainControl(method = "cv", number = 100))
te = Sys.time()
te - ts

```




## 1. Random Forest

Random Forest is an ensemble learning method used for both classification and regression. It operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.

### 1.1 Key Points

- Random Forests build multiple decision trees and merge them together to get a more accurate and stable prediction.
- One big advantage of Random Forest is that it can be used for both classification and regression tasks.
- It's less likely to overfit than a decision tree.
- Important parameters: number of trees, depth of trees, minimum samples per leaf node.

### 1.2 Implementing Random Forest in R

We use the `randomForest` package in R. Here's a basic example using the Iris dataset:

```{r example, message=FALSE}

# Load the Iris dataset
data(iris)

# Splitting data 
set.seed(1204)
train_index <- createDataPartition(iris$Species, 
                                  times = 1,
                                  p = 0.5, 
                                  list = FALSE)
train_set <- iris[train_index, ]
test_set <- iris[-train_index, ]

# Fit a Random Forest model
rf_model <- randomForest(Species ~ ., data=train_set, ntree=100)

# Print the model summary
print(rf_model)

# Model evaluation
pred <- predict(rf_model, newdata = test_set, type = "class")
confusionMatrix(pred, test_set$Species)


```


### 1.3 Model Tuning
Model tuning involves adjusting parameters like the number of trees (`ntree`) and the number of variables randomly sampled as candidates at each split (`mtry`).

### 1.4 Interpreting the Results

Random Forest models can be evaluated using various metrics. For classification, confusion matrices and accuracy are commonly used. For regression, metrics like RMSE (Root Mean Squared Error) are typical.

**Variable Importance**
One useful feature of Random Forests is the ability to rank variables by importance.
```{r vi, message=FALSE}
# Variable Importance
importance(rf_model)
varImpPlot(rf_model)
```
Note:

- The `MeanDecreaseGini` for a feature is calculated by averaging the decreases in Gini impurity across all trees in the forest due to splits on that feature.
- A higher `MeanDecreaseGini` value for a feature means that, on average, splitting the dataset on this feature has led to a greater decrease in impurity across the trees in the forest. In other words, the feature is more important for classifying the data correctly.




## 2. Today's lecture
- Today: regularization (ridge regression, lasso)
- Lab this Friday, Dec 8 (decision trees, random forest) 
- Homework 4, Project, Project Peer Evaluation due on the same day

