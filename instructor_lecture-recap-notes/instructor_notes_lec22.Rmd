---
title: "instructor recap notes - lec 22"
date: "2023-11-08"
output: html_document
---

```{r setup, message=FALSE}
# load the required libraries
library(tidyverse)
library(ggplot2)
library(dslabs)
library(broom)
```
### Random quiz
Why do we need to set a seed (`set.seed()`) when we submit the code used to show how well our proposed algorithm works? 

- Simulation: `sample()`
- Create data partition `createDataPartition()`
- Built-in random number generation: `rnorm()`

**Tips of the day**

1. get quick summary statistics for a variable or all of the variables in your dataset
```{r tips}
set.seed(1108)
a <- rnorm(100)
summary(a)

data(murders)
summary(murders)

```


## 1. Recap Notes: Machine learning framework


### Introduction to Machine Learning (ML)

- ML has many applications, such as handwriting recognition, speech recognition (e.g., Siri), recommendation systems, and autonomous vehicles.
- Distinction between AI and ML: AI started with rule-based decision-making while ML decisions are derived from data.
- Distinction between ML and regression or predictive modeling: 

### General set-up / workflow
- ML models are evaluated based on their performance with new, unseen data.
- Training and test sets are used to develop and test models respectively.

### Notation in ML

- Outcomes (denoted by \( Y \)) are what we want to predict.
- Features (denoted by \( X_1, X_2, \ldots, X_p \)) are the data we use to predict outcomes.
- ML algorithms are trained with known outcomes to predict future unknown outcomes.

### Types of outcomes to be predicted

- Categorical outcomes involve classification into classes (e.g., spam or not spam).
- Continuous outcomes involve prediction of a quantitative measure (e.g., blood pressure, change of scores in cognitive test before and after treatment).


### Case study example : Digit Reader

- Machine learning algorithms are used for reading zip codes automatically in post offices.
- The training set involves known outcomes (e.g., digits on envelopes).
- Features are pixel intensities from images.

### Evaluation metrics in ML (classification) 

- Overall accuracy measures the proportion of correct predictions.
- Confusion matrices help understand the detailed performance of models.
- Sensitivity (True Positive Rate) and Specificity (True Negative Rate) are considered alongside accuracy to evaluate models better.
- ... and more

## Simple example: predicting sex using height

- A simple model predicts sex based on height with a certain accuracy.
- Overall accuracy alone can be deceptive; it's important to also consider sensitivity and specificity.
- Prevalence can bias accuracy; therefore, it's critical to evaluate models with balanced metrics.
- Sensitivity (ability to predict positive when the outcome is positive) and specificity (ability to predict negative when the outcome is negative) offer a more nuanced view of model performance.

```{r confusion m, message=FALSE}
# Load necessary libraries
library(caret)
library(dslabs)

# Load data
data(heights)

# Define the outcome and predictors
y <- heights$sex
x <- heights$height

# Split the data into training and test sets
set.seed(2007) # Setting seed for reproducibility
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
train_set <- heights[-test_index, ]
test_set <- heights[test_index, ]

# Define a simple prediction rule based on a chosen cutoff
cutoff <- 68 # Example cutoff for height to predict Male or Female
y_hat <- ifelse(test_set$height > cutoff, "Male", "Female") %>%
  factor(levels = levels(test_set$sex))

# Create the confusion matrix
conf_matrix <- confusionMatrix(data = y_hat, reference = test_set$sex)

# Display the confusion matrix
print(conf_matrix$table)

# Display overall accuracy
print(conf_matrix$overall['Accuracy'])

# Display sensitivity and specificity
print(conf_matrix$byClass['Sensitivity'])
print(conf_matrix$byClass['Specificity'])

# If you want to access the Positive Predictive Value (Precision) and Negative Predictive Value
ppv <- conf_matrix$byClass['Pos Pred Value']
npv <- conf_matrix$byClass['Neg Pred Value']
print(ppv)
print(npv)
```


## 2. Today's lecture
- Midterm released Nov 8 (due Nov 22) **no free late days**
  - We encourage you to **email** TF / instructor the questions you have about Midterms
  - We will post a Google doc with all of the questions students have asked about midterm so we all have the same information
- Final project teams created: for those who didn't submit the preference form, we have assigned you to a team with a project (generated by random number generator!). 
  - Office hours for the following two weeks are (mostly) designated for team meetings
  - Find a time (office hours preferably) to meet with TF (20 minutes) before Nov 17 to review your project (preferably during office hours). For meetings at other times, please email TF or [schedule here](https://calendly.com/lukebenz-gei/20min?month=2023-11).
- Today: Machine learning intro (evaluation metrics, logistic regression)

